{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamurAIGPT/LlamaIndex-course/blob/main/query_engines/Query_Engines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICcKuSQWd9uh"
      },
      "source": [
        "# Query Engines\n",
        "\n",
        "Query Engine works on top of retriever and response synthesizer which we discussed in our previous lessons\n",
        "\n",
        "A query engine allows you to ask question over your data. LlamaIndex supports a bunch of query engines. We will be discussing some of them now\n",
        "\n",
        "1. Router Query Engine\n",
        "2. Retriever Router Query Engine\n",
        "3. Joint QA Summary Query Engine\n",
        "4. Sub Question Query Engine\n",
        "5. Custom Retriever with Hybrid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gvzm39FGgki8"
      },
      "source": [
        "### Router Query Engine\n",
        "\n",
        "Router Query Engine helps you create a query engine over multiple indices. An index is chosen based on the similarity of the question with index description\n",
        "\n",
        "For example if you have an index knowledgeable on Maths and another index knowledgeable in Physics, you can create a combined query enginer over both these indices by creating a Router Query Engine\n",
        "\n",
        "Let's try to understand with the help of an example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWpuvOgPd5Fs"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "a4v2RkfKhO5b"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "openai.api_key = \"your-openai-key\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m169xw8WhYpg"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().handlers = []\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "from llama_index import (\n",
        "    VectorStoreIndex,\n",
        "    ListIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    ServiceContext,\n",
        "    StorageContext,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9s80nf9h8TR"
      },
      "source": [
        "### Download relevant data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1T7MPyvhfXc",
        "outputId": "e20bc493-66da-4425-afe3-bf42c8733923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-07-22 16:56:24--  https://github.com/jerryjliu/llama_index/blob/main/examples/paul_graham_essay/data/paul_graham_essay.txt\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84944 (83K) [text/plain]\n",
            "Saving to: ‘paul_graham_essay.txt’\n",
            "\n",
            "\rpaul_graham_essay.t   0%[                    ]       0  --.-KB/s               \rpaul_graham_essay.t 100%[===================>]  82.95K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-07-22 16:56:25 (5.35 MB/s) - ‘paul_graham_essay.txt’ saved [84944/84944]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "!wget https://github.com/jerryjliu/llama_index/blob/main/examples/paul_graham_essay/data/paul_graham_essay.txt\n",
        "!mv paul_graham_essay.txt data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ntoicl2Yh6bh"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(\"./data/\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i9r25o7XiBgo"
      },
      "outputs": [],
      "source": [
        "service_context = ServiceContext.from_defaults(chunk_size=1024)\n",
        "nodes = service_context.node_parser.get_nodes_from_documents(documents)\n",
        "storage_context = StorageContext.from_defaults()\n",
        "storage_context.docstore.add_documents(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sc47wyWYiC1Y"
      },
      "outputs": [],
      "source": [
        "list_index = ListIndex(nodes, storage_context=storage_context)\n",
        "vector_index = VectorStoreIndex(nodes, storage_context=storage_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJQvNyqxpmjY"
      },
      "source": [
        "We are creating 2 query engines, one for summarization tasks and one for QA\n",
        "\n",
        "For summarization we are using a list index with response mode tree_summarize which can build a tree data structure from input data and create a summary of the data bottom-up\n",
        "\n",
        "For QA we will be using VectorIndex which can fetch the top relevant documents for creating the response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "u1v0jsn0iSV0"
      },
      "outputs": [],
      "source": [
        "from llama_index.tools.query_engine import QueryEngineTool\n",
        "list_query_engine = list_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True,\n",
        ")\n",
        "vector_query_engine = vector_index.as_query_engine()\n",
        "list_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=list_query_engine,\n",
        "    description=\"Useful for summarization questions related to Paul Graham eassy on What I Worked On.\",\n",
        ")\n",
        "\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=\"Useful for retrieving specific context from Paul Graham essay on What I Worked On.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVdXJfMcqB0w"
      },
      "source": [
        "Now we will build a RouterQueryEngine on top of these two which can select one of these engines based on the input query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HB_n-65oiyJl"
      },
      "outputs": [],
      "source": [
        "from llama_index.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.selectors.llm_selectors import LLMSingleSelector, LLMMultiSelector\n",
        "from llama_index.selectors.pydantic_selectors import (\n",
        "    PydanticMultiSelector,\n",
        "    PydanticSingleSelector,\n",
        ")\n",
        "\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=PydanticSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        list_tool,\n",
        "        vector_tool,\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-xAWjHOqPto"
      },
      "source": [
        "Since the question is a summarization question, it picks up the list index and creates a summary to give a response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WcCcJ7Ri0-f",
        "outputId": "5f25eb57-1221-4140-d49d-e5ac573e6612"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The document is a collection of essays written by Paul Graham, reflecting on his journey from working on Viaweb to starting Y Combinator, painting, writing essays, working on Lisp, and writing Bel. He discusses topics such as the difficulty of carrying heavy items, the problems with running a forum and writing essays, leaving Y Combinator, and the concept of invented versus discovered. He also thanks several people for reading drafts of the essays.\n"
          ]
        }
      ],
      "source": [
        "print(query_engine.query(\"What is the summary of the document?\").response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxnX1KLJqZv9"
      },
      "source": [
        "Since here we have a QA question, a vector index is chosen to give the appropriate response. Thus based on the input query the proper query engine is selected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiRSLr_8pW6N",
        "outputId": "2a7f1413-25d5-4833-b26b-d6e1c0869b92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Paul Graham decided to paint. He wanted to see how good he could get if he dedicated himself to painting and left his job at Y Combinator. He recruited Dan Giffin and two undergrads to help him build a web app for making web apps, and he moved to Cambridge to start the company.\n"
          ]
        }
      ],
      "source": [
        "print(query_engine.query(\"What did Paul Graham do after RICS?\").response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqdR-jevqraG"
      },
      "source": [
        "### Retriever Router Query Engine\n",
        "\n",
        "Retriever Router Query Engine functions similar to the above in functionality. The only difference being the Router is powered by a retriever\n",
        "\n",
        "The advantage of using a vector index powered retriever is, the number of query engines that can be kept as part of the router is no longer limited by model context length. Thus any number of query engines can be used as part of the retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "P_uz6d3Ppfuy"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "gAKwsvwarXaT"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "from llama_index import (\n",
        "    VectorStoreIndex,\n",
        "    ListIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    ServiceContext,\n",
        "    StorageContext,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GHdjIi7AraPj"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(\"./data/\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RJQGc79MreWr"
      },
      "outputs": [],
      "source": [
        "service_context = ServiceContext.from_defaults(chunk_size=1024)\n",
        "nodes = service_context.node_parser.get_nodes_from_documents(documents)\n",
        "storage_context = StorageContext.from_defaults()\n",
        "storage_context.docstore.add_documents(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FVMo6LqOrh6g"
      },
      "outputs": [],
      "source": [
        "list_index = ListIndex(nodes, storage_context=storage_context)\n",
        "vector_index = VectorStoreIndex(nodes, storage_context=storage_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OYAiZ47trkE2"
      },
      "outputs": [],
      "source": [
        "from llama_index.tools.query_engine import QueryEngineTool\n",
        "\n",
        "list_query_engine = list_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\", use_async=True\n",
        ")\n",
        "vector_query_engine = vector_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\", use_async=True\n",
        ")\n",
        "\n",
        "list_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=list_query_engine,\n",
        "    description=\"Useful for questions asking for a biography of the author.\",\n",
        ")\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=\"Useful for retrieving specific snippets from the author's life, like his time in college, his time in YC, or more.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVhMVcXusvET"
      },
      "source": [
        "Until now the procedure is same as the Router Query Engine, we create 2 query engines. Now here is the difference, we build a Router Query Engine using a vector index which is ObjectIndex mentioned here.\n",
        "\n",
        "ObjectIndex is an underlying index data structure and can serialize QueryEngineTool objects to indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lHJ50b4PrnDZ"
      },
      "outputs": [],
      "source": [
        "from llama_index import VectorStoreIndex\n",
        "from llama_index.objects import ObjectIndex, SimpleToolNodeMapping\n",
        "\n",
        "tool_mapping = SimpleToolNodeMapping.from_objects([list_tool, vector_tool])\n",
        "obj_index = ObjectIndex.from_objects(\n",
        "    [list_tool, vector_tool],\n",
        "    tool_mapping,\n",
        "    VectorStoreIndex,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Hl7dII5rrrFp"
      },
      "outputs": [],
      "source": [
        "from llama_index.query_engine import ToolRetrieverRouterQueryEngine\n",
        "\n",
        "query_engine = ToolRetrieverRouterQueryEngine(obj_index.as_retriever())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EG8axa9ArtES"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\"What is a biography of the author's life?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8OLDIZzruvA",
        "outputId": "7cb6eba3-aaa0-4d9f-ca9b-f692c481b76d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Paul Graham is a computer scientist, programmer, and entrepreneur who was born in England. He moved to the United States in the 1980s to pursue a PhD in computer science at Harvard University, where he wrote the book On Lisp and worked on a project called Bel. He then moved to Italy to study art at the Accademia di Belli Arti in Florence. After returning to the United States, he wrote essays and eventually moved back to England with his family. In 2019, he finished Bel and wrote a series of essays. In 2020, he wrote an essay about how he chooses what to work on.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UZgQGkrLr1-v"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\"What did Paul Graham do during his time in college?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91M576qmr7mW",
        "outputId": "8cc6e0b3-a2be-4ee9-fbd9-9d072feb1734"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Paul Graham studied computer science and took art classes at Harvard University. He applied to two art schools, RISD and the Accademia di Belli Arti in Florence, and was accepted to RISD. He wrote a dissertation on applications of continuations in order to graduate from Harvard. He then attended RISD, where he took the foundation classes in drawing, color, and design. He also took the entrance exam for the Accademia di Belli Arti in Florence and passed.\n"
          ]
        }
      ],
      "source": [
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbY29gsTt0RJ"
      },
      "source": [
        "### Sub Question Query Engine\n",
        "\n",
        "A sub question query engine tackles the problem of answering a complex query\n",
        "by breaking down the complex query into sub questions for each relevant data source, then gather all the intermediate reponses and synthesizes a final response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NduxyFdXr-hN"
      },
      "outputs": [],
      "source": [
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.query_engine import SubQuestionQueryEngine\n",
        "from llama_index.callbacks import CallbackManager, LlamaDebugHandler\n",
        "from llama_index import ServiceContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "aKt5Y5o7uWwU"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "pg_essay = SimpleDirectoryReader(input_dir=\"./data/\").load_data()\n",
        "\n",
        "# build index and query engine\n",
        "query_engine = VectorStoreIndex.from_documents(pg_essay).as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "zwacdDsnuaqg"
      },
      "outputs": [],
      "source": [
        "query_engine_tools = [\n",
        "    QueryEngineTool(\n",
        "        query_engine=query_engine,\n",
        "        metadata=ToolMetadata(\n",
        "            name=\"pg_essay\", description=\"Paul Graham essay on What I Worked On\"\n",
        "        ),\n",
        "    )\n",
        "]\n",
        "\n",
        "# Using the LlamaDebugHandler to print the trace of the sub questions\n",
        "# captured by the SUB_QUESTION callback event type\n",
        "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
        "callback_manager = CallbackManager([llama_debug])\n",
        "service_context = ServiceContext.from_defaults(callback_manager=callback_manager)\n",
        "query_engine = SubQuestionQueryEngine.from_defaults(\n",
        "    query_engine_tools=query_engine_tools,\n",
        "    service_context=service_context,\n",
        "    use_async=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yIDxeppuesE",
        "outputId": "b698ca00-2f40-4f17-e635-84a5b99db81b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 2 sub questions.\n",
            "\u001b[36;1m\u001b[1;3m[pg_essay] Q: What did Paul Graham work on before YC?\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m[pg_essay] Q: What did Paul Graham work on after YC?\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m[pg_essay] A: \n",
            "Paul Graham continued to work on writing essays and working on YC. He also worked on Hacker News, which was originally meant to be a news aggregator for startup founders and was called Startup News. He wrote all of YC's internal software in Arc, but gradually stopped working on Arc due to lack of time and the infrastructure depending on it.\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m[pg_essay] A: \n",
            "Before YC, Paul Graham worked on hacking, writing essays, and Arc, a programming language. He also ran a weekly dinner at his building in Cambridge and created the Summer Founders Program, which invited undergrads to apply for startup funding.\n",
            "\u001b[0m**********\n",
            "Trace: query\n",
            "    |_llm ->  3.388304 seconds\n",
            "    |_sub_questions ->  3.653011 seconds\n",
            "    |_synthesize ->  3.346864 seconds\n",
            "      |_llm ->  3.342244 seconds\n",
            "**********\n"
          ]
        }
      ],
      "source": [
        "response = await query_engine.aquery(\n",
        "    \"How was Paul Grahams life different before and after YC?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCkuvlChuhFj",
        "outputId": "4fe8f02f-f662-46ce-880b-99bead97a29b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Paul Graham's life changed significantly after YC. Before YC, he was mainly focused on hacking, writing essays, and developing Arc, a programming language. He also ran a weekly dinner at his building in Cambridge and created the Summer Founders Program. After YC, he continued to write essays and work on YC, but he also worked on Hacker News and wrote all of YC's internal software in Arc. He gradually stopped working on Arc due to lack of time and the infrastructure depending on it.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AVI6Po_u2XI"
      },
      "source": [
        "### Joint QA Summary Query Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "SmVp-TytusrT"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "q8OaRq46u4zW"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "3zen7XaFu6Y0"
      },
      "outputs": [],
      "source": [
        "from llama_index.composability.joint_qa_summary import QASummaryQueryEngineBuilder\n",
        "from llama_index import SimpleDirectoryReader, ServiceContext, LLMPredictor\n",
        "from llama_index.response.notebook_utils import display_response\n",
        "from llama_index.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "CMGBt9u5u8PL"
      },
      "outputs": [],
      "source": [
        "reader = SimpleDirectoryReader(\"./data/\")\n",
        "documents = reader.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "U6bzq6EEvAHk"
      },
      "outputs": [],
      "source": [
        "service_context = ServiceContext.from_defaults(chunk_size=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "BNVuJNZcvJqe"
      },
      "outputs": [],
      "source": [
        "query_engine_builder = QASummaryQueryEngineBuilder(service_context=service_context)\n",
        "query_engine = query_engine_builder.build_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "zciG7c6XvPHx"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\n",
        "    \"Can you give me a summary of the author's life?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeGPyRb_vTcB",
        "outputId": "236fa563-f786-412d-f80a-9199426be795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The author, Paul Graham, is a computer scientist and artist. He was born in England and moved to the US to pursue a PhD in computer science. While in grad school, he worked on On Lisp and wrote a dissertation on applications of continuations. He then applied to art schools and was accepted to RISD and the Accademia di Belli Arti in Florence. He moved to Florence and passed the entrance exam, and then spent 3 months writing essays. He then worked on Bel, an interpreter written in itself, for years, while living in England. In 2019, Bel was finished and he wrote essays about topics he had stacked up. He now lives in England and is thinking about what to work on next.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Rsa1FrwdvVmB"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\n",
        "    \"What did the author do during his time in art school?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gef9lLFvalP",
        "outputId": "26f7517d-80c9-47e2-f72b-0958500e573d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The author took art classes at Harvard, applied to two art schools (RISD and the Accademia di Belli Arti in Florence), took the entrance exam for the Accademia di Belli Arti in Florence, attended the RISD foundation program, and painted still lives in his bedroom at night while attending the Accademia di Belli Arti in Florence. He also learned Italian and studied under professor Ulivi.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkjOnu85vl0T"
      },
      "source": [
        "### Custom Retriever with Hybrid Search\n",
        "\n",
        "Keyword based search was the initial form of search used in information retrieval systems. Then recently we have Vector db based search which works based on semantic similarity.\n",
        "\n",
        "It is not always necessary that a Vector db backed search performs better than a keyword based search on a particular query. It can be vice-versa.\n",
        "\n",
        "Thus to overcome this, we can use Hybrid search which results in best of both worlds. Let's discuss how we can achieve this with the help of a Custom Retreiver in LlamaIndex\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ytqxBpDl1bS9"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "from llama_index import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleKeywordTableIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    ServiceContext,\n",
        "    StorageContext,\n",
        ")\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "NvLaGasX1fgs"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(\"./data/\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "tr3ju5Tr1i2w"
      },
      "outputs": [],
      "source": [
        "service_context = ServiceContext.from_defaults(chunk_size=1024)\n",
        "node_parser = service_context.node_parser\n",
        "nodes = node_parser.get_nodes_from_documents(documents)\n",
        "storage_context = StorageContext.from_defaults()\n",
        "storage_context.docstore.add_documents(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWaHLCnj1npS",
        "outputId": "c4622cae-2fc3-4d7c-a1c6-6d0de52f55c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "vector_index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
        "keyword_index = SimpleKeywordTableIndex(nodes, storage_context=storage_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "I4MyW7MN1q87"
      },
      "outputs": [],
      "source": [
        "from llama_index import QueryBundle\n",
        "\n",
        "# import NodeWithScore\n",
        "from llama_index.schema import NodeWithScore\n",
        "\n",
        "# Retrievers\n",
        "from llama_index.retrievers import (\n",
        "    BaseRetriever,\n",
        "    VectorIndexRetriever,\n",
        "    KeywordTableSimpleRetriever,\n",
        ")\n",
        "\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "SMXnlszC1uBY"
      },
      "outputs": [],
      "source": [
        "class CustomRetriever(BaseRetriever):\n",
        "    \"\"\"Custom retriever that performs both semantic search and hybrid search.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vector_retriever: VectorIndexRetriever,\n",
        "        keyword_retriever: KeywordTableSimpleRetriever,\n",
        "        mode: str = \"AND\",\n",
        "    ) -> None:\n",
        "        \"\"\"Init params.\"\"\"\n",
        "\n",
        "        self._vector_retriever = vector_retriever\n",
        "        self._keyword_retriever = keyword_retriever\n",
        "        if mode not in (\"AND\", \"OR\"):\n",
        "            raise ValueError(\"Invalid mode.\")\n",
        "        self._mode = mode\n",
        "\n",
        "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
        "        \"\"\"Retrieve nodes given query.\"\"\"\n",
        "\n",
        "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
        "        keyword_nodes = self._keyword_retriever.retrieve(query_bundle)\n",
        "\n",
        "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
        "        keyword_ids = {n.node.node_id for n in keyword_nodes}\n",
        "\n",
        "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
        "        combined_dict.update({n.node.node_id: n for n in keyword_nodes})\n",
        "\n",
        "        if self._mode == \"AND\":\n",
        "            retrieve_ids = vector_ids.intersection(keyword_ids)\n",
        "        else:\n",
        "            retrieve_ids = vector_ids.union(keyword_ids)\n",
        "\n",
        "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
        "        return retrieve_nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "M7pvpGCS1yQJ"
      },
      "outputs": [],
      "source": [
        "from llama_index import get_response_synthesizer\n",
        "from llama_index.query_engine import RetrieverQueryEngine\n",
        "\n",
        "# define custom retriever\n",
        "vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=2)\n",
        "keyword_retriever = KeywordTableSimpleRetriever(index=keyword_index)\n",
        "custom_retriever = CustomRetriever(vector_retriever, keyword_retriever)\n",
        "\n",
        "# define response synthesizer\n",
        "response_synthesizer = get_response_synthesizer()\n",
        "\n",
        "# assemble query engine\n",
        "custom_query_engine = RetrieverQueryEngine(\n",
        "    retriever=custom_retriever,\n",
        "    response_synthesizer=response_synthesizer,\n",
        ")\n",
        "\n",
        "# vector query engine\n",
        "vector_query_engine = RetrieverQueryEngine(\n",
        "    retriever=vector_retriever,\n",
        "    response_synthesizer=response_synthesizer,\n",
        ")\n",
        "# keyword query engine\n",
        "keyword_query_engine = RetrieverQueryEngine(\n",
        "    retriever=keyword_retriever,\n",
        "    response_synthesizer=response_synthesizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "VRuQY4wl10_Q"
      },
      "outputs": [],
      "source": [
        "response = custom_query_engine.query(\"What did the author do during his time at YC?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSStcZ_c127F",
        "outputId": "84511db8-4acf-4921-b27c-f5efb271e7ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The author worked on YC, writing essays, developing internal software in Arc, and creating Hacker News. He also helped select and support founders, resolve disputes between cofounders, and fight with people who maltreated the startups. He worked hard, even at the parts he didn't like, and eventually handed YC over to someone else. After his mother's death, he checked out of YC and decided to pursue painting.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNgytkgE1PfjiuoA/fJLGnA",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
