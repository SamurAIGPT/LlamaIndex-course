{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkOGY71LuN59nYLfcKK6Y6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamurAIGPT/LlamaIndex-course/blob/main/retrievers/Retrievers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index"
      ],
      "metadata": {
        "id": "ve7IycYJxhzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrievers\n",
        "\n",
        "In LlamaIndex retrievers are responsible for fetching the most relevant context given a user query. A retriever is built on top of an index and specifies how the nodes needs to be fetched from an index. We have discussed indices in the previous lesson\n",
        "\n",
        "LlamaIndex supports many types of retrievers, including the following:\n",
        "\n",
        "**Vector Store Retriever**\n",
        "\n",
        "The Vector Store Retriever fetches the top-k most similar nodes from a vector store index. Mode has no significance here\n",
        "\n",
        "**List Retriever**\n",
        "\n",
        "You can use the List Retriever to fetch all nodes from a list index. This retriever supports two modes: default and embeddings.\n",
        "\n",
        "Default mode fetches all the nodes while embeddings mode fetches top-k nodes using embeddings\n",
        "\n",
        "**Tree Retriever**\n",
        "\n",
        "The Tree Retriever, as its name suggests, extracts nodes from a hierarchical tree of nodes. This retriever supports many different modes, the default is select_leaf.\n",
        "\n",
        "**Keyword Table Retriever**\n",
        "\n",
        "The Keyword Table Retriever extracts keywords from the query and uses them to find nodes having matching keywords.\n",
        "\n",
        "The retriever supports three different modes: default, simple, and rake.\n",
        "\n",
        "**Knowledge Graph Retriever**\n",
        "\n",
        "The Knowledge Graph Retriever fetches nodes from a hierarchical tree of nodes.\n",
        "\n",
        "It supports keywords, embeddings, and hybrid mode. Hybrid mode uses both keywords and embeddings to find relevant triplets.\n",
        "\n",
        "Here is a code example of a retriever built on a list index"
      ],
      "metadata": {
        "id": "Kt4zn-0uo0Ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import ListIndex\n",
        "from llama_index import download_loader\n",
        "\n",
        "YoutubeTranscriptReader = download_loader(\"YoutubeTranscriptReader\")\n",
        "\n",
        "loader = YoutubeTranscriptReader()\n",
        "docs = loader.load_data(ytlinks=['https://www.youtube.com/watch?v=nHcbHdgVUJg&ab_channel=WintWealth'])\n",
        "list_index = ListIndex(docs)\n",
        "retriever = list_index.as_retriever(\n",
        "    retriever_mode='embedding',\n",
        ")"
      ],
      "metadata": {
        "id": "nfVxTBWCyCr_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Node postprocessor\n",
        "\n",
        "Node postprocessors in LlamaIndex take a set of nodes and apply a transformation or filtering before returning them.\n",
        "\n",
        "Node postprocessors are most commonly used in a query engine, after the retrieval step and before the response synthesis step.\n",
        "\n",
        "For instance, you can require specific keywords to be absent or present in the retrieved nodes. You can also rank results based on attributes such as time.\n",
        "\n",
        "There are many types of post-processors, let's discuss few of them:\n",
        "\n",
        "**SimilarityPostprocessor**\n",
        "\n",
        "Allows you to require the retrieved nodes to have a minimum similarity score.\n",
        "\n",
        "**KeywordNodePostprocessor**\n",
        "\n",
        "Allows you to require certain keywords to be present in nodes.\n",
        "\n",
        "**TimeWeightedPostprocessor**\n",
        "\n",
        "The TimeWeightedPostprocessor ranks the nodes by their recency.\n",
        "\n",
        "**PIINodePostprocessor**\n",
        "\n",
        "The PIINodeprocessor masks Personally Identifiable Information (PII) in the text using a local LLM such as StableLM. For example, replaces a credit card number with [CREDIT_CARD_NUMBER].\n",
        "\n",
        "**FixedRecencyPostprocessor**\n",
        "\n",
        "If the query involves a time factor (temporal query), this post processor orders the nodes by date and returns the first k nodes to the response synthesizes.\n",
        "\n",
        "Let's see a code example of a SimilarityPostprocessor"
      ],
      "metadata": {
        "id": "gcj2grlkv6ly"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Tit8oplEngob"
      },
      "outputs": [],
      "source": [
        "from llama_index.indices.postprocessor import SimilarityPostprocessor\n",
        "from llama_index.schema import Node, NodeWithScore\n",
        "\n",
        "nodes = [\n",
        "  NodeWithScore(node=Node(text=\"text\"), score=0.7),\n",
        "  NodeWithScore(node=Node(text=\"text\"), score=0.8)\n",
        "]\n",
        "\n",
        "# filter nodes below 0.75 similarity score\n",
        "processor = SimilarityPostprocessor(similarity_cutoff=0.75)\n",
        "filtered_nodes = processor.postprocess_nodes(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_nodes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfXNwYC4ze69",
        "outputId": "a005f858-e678-4c6b-9fca-db01fabe93a3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[NodeWithScore(node=TextNode(id_='45c4ba0f-edac-4b2b-bcaa-ae9dd3d5210a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='8a363c3e689f3fd0f4d742d70375383ef3906cc7a90e6f9ee402405a72de759c', text='text', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.8)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen, node with score 0.7 is filtered out"
      ],
      "metadata": {
        "id": "8VQLiyJI04Yd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Response Synthesizer\n",
        "\n",
        "A response synthesizer takes a list of nodes as input and generates output in the form of a response.\n",
        "\n",
        "LlamaIndex supports different modes of response synthesis, including:\n",
        "\n",
        "**Refine**\n",
        "\n",
        "An iterative way of generating a response that uses all nodes. The initial answer is generated using the first node as context. This answer and the second node are then fed as context to a refinement prompt. This process continues until all nodes have been processed in the same way and the final response has been generated.\n",
        "\n",
        "**Compact and Refine**\n",
        "\n",
        "Same as refine, except the text chunks are merged into larger chunks to optimize cost and performance.\n",
        "\n",
        "**Tree Summarize**\n",
        "\n",
        "Tree summarization is a bottoms-up approach to constructing a response from a list of nodes. It involves generating a summary and parent node by combining every two nodes until only one answer remains.\n",
        "\n",
        "**Simple Summarize**\n",
        "\n",
        "Combines all text chunks and makes one LLM call using the whole text as context."
      ],
      "metadata": {
        "id": "j6UjRIsB0_L7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import (\n",
        "    VectorStoreIndex,\n",
        "    get_response_synthesizer,\n",
        ")\n",
        "from llama_index.retrievers import VectorIndexRetriever\n",
        "from llama_index.query_engine import RetrieverQueryEngine\n",
        "\n",
        "# build index\n",
        "index = VectorStoreIndex.from_documents(docs)\n",
        "\n",
        "# configure retriever\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=2,\n",
        ")\n",
        "\n",
        "# configure response synthesizer\n",
        "response_synthesizer = get_response_synthesizer(\n",
        "    response_mode=\"tree_summarize\",\n",
        ")\n",
        "\n",
        "# assemble query engine\n",
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever=retriever,\n",
        "    response_synthesizer=response_synthesizer,\n",
        ")\n",
        "\n",
        "# query\n",
        "response = query_engine.query(\"What did the author do growing up?\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "Wz0AvDHN01Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rNX2_3H_14d8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}